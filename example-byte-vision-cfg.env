
### Global app folder, path settings ###
AppLogPath=/byte-vision-mcp/logs/
AppLogFileName=/byte-vision-mcp.log
PromptCachePath=/byte-vision-mcp/prompt-cache/
ModelPath=/byte-vision-mcp/models/
LLamaCliPath=/byte-vision-mcp/llamacpp/llama-cli.exe
HttpPort=:8080
EndPoint=/mcp-completion
TimeOutSeconds=300

### Default llama-cli settings - Reordered to match help output ###
Description=Default

# ----- common params -----

# -t, --threads N - number of threads to use during generation (default: -1)
ThreadsCmd=--threads
ThreadsVal=8

# -tb, --threads-batch N - number of threads to use during batch and prompt processing (default: same as --threads)
ThreadsBatchCmd=--threads-batch
ThreadsBatchVal=8

# -c, --ctx-size N - size of the prompt context (default: 4096, 0 = loaded from model)
CtxSizeCmd=--ctx-size
CtxSizeVal=40960

# -n, --predict, --n-predict N - number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)
PredictCmd=--n-predict
PredictVal=2560

# -b, --batch-size N - logical maximum batch size (default: 2048)
BatchCmd=--batch-size
BatchCmdVal=2048

# -ub, --ubatch-size N - physical maximum batch size (default: 512)
UBatchCmd=--ubatch-size
UBatchCmdVal=512

# --keep N - number of tokens to keep from the initial prompt (default: 0, -1 = all)
KeepCmd=--keep
KeepVal=-1

# -fa, --flash-attn - enable Flash Attention (default: disabled)
FlashAttentionCmd=--flash-attn
FlashAttentionCmdEnabled=true

# -p, --prompt PROMPT - prompt to start generation with; for system message, use -sys
PromptCmd=--prompt
PromptCmdEnabled=true

# -f, --file FNAME - a file containing the prompt (default: none)
PromptFileCmd=--file
PromptFileVal=

# -e, --escape - process escapes sequences (\n, \r, \t, \', \", \\) (default: true)
EscapeNewLinesCmd=-e
EscapeNewLinesCmdEnabled=true

# ----- model params -----

# -m, --model FNAME - model path (default: models/7B/ggml-model-f16.gguf)
ModelCmd=--model
ModelFullPathVal=/byte-vision-mcp/models/Qwen3-8B-Q8_0.gguf

# -ngl, --n-gpu-layers N - number of layers to store in VRAM
GPULayersCmd=--n-gpu-layers
GPULayersVal=33

# -mg, --main-gpu N - the GPU to use for the model (with split-mode = none), or for intermediate results and KV (with split-mode = row) (default: 0)
MainGPUCmd=--main-gpu
MainGPUVal=2

# ----- sampling params -----

# --temp N - temperature (default: 0.8)
TemperatureCmd=--temp
TemperatureVal=0.6

# --top-k N - top-k sampling (default: 40, 0 = disabled)
TopKCmd=--top-k
TopKVal=20

# --top-p N - top-p sampling (default: 0.9, 1.0 = disabled)
TopPCmd=--top-p
TopPVal=0.95

# --min-p N - min-p sampling (default: 0.1, 0.0 = disabled)
MinPCmd=--min-p
MinPVal=0

# --repeat-penalty N - penalize repeat sequence of tokens (default: 1.1, 1.0 = disabled)
RepeatPenaltyCmd=--repeat-penalty
RepeatPenaltyVal=1

# --repeat-last-n N - last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size)
RepeatLastPenaltyCmd=--repeat-last-n
RepeatLastPenaltyVal=64

# ----- other params -----

# --prompt-cache FNAME - file to cache prompt state for faster startup (default: none)
PromptCacheCmd=--prompt-cache
PromptCacheVal=/byte-vision-mcp/prompt-cache/Qwen3-8B-Q8_0

# --log-file FNAME - specify a log filename (default: disabled)
ModelLogFileCmd=--log-file
ModelLogFileNameVal=/byte-vision-mcp/logs/Qwen3-8B-Q8_0.log

# --no-display-prompt - don't print the prompt
NoDisplayPromptCmd=--no-display-prompt
NoDisplayPromptEnabled=true

# --seed N - RNG seed (default: -1, use random seed for < 0)
RandomSeedCmd=--seed
RandomSeedCmdVal=112358